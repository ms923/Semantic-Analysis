# Theory

This code implements a machine learning pipeline to assess semantic textual similarity between two 
sentences. It does so by using a Support Vector Regressor (SVR) on cosine similarity scores calculated 
from sentence embeddings generated by a Word2Vec model. 

Absolutely, let’s break down these terms in a simpler way.

1. SVR (Support Vector Regression)
SVR stands for Support Vector Regression, which is a type of machine learning model. Think of it as a tool that tries to find a line (or a curve) that best matches the relationship between two things — in this case, the relationship between two sentences and how similar they are.

In this code, we use SVR to predict how similar two sentences are based on a “similarity score.” SVR is like drawing a line through points on a graph, where each point represents a pair of sentences and their similarity. The goal of SVR is to make accurate predictions of similarity for new sentences it hasn’t seen before.

2. Embedding
Embeddings are a way to represent words or sentences as numbers, which is important because machine learning models work with numbers, not text. An embedding converts each word or sentence into a list of numbers, which captures its meaning.

Imagine each word as a point in a big, 100-dimensional space (like a huge graph with 100 directions instead of just the usual 2D or 3D). Words with similar meanings end up close to each other in this space. For example, “dog” and “puppy” might be close, while “dog” and “computer” are far apart.

In this code, we use sentence embeddings by averaging the embeddings for each word in the sentence. This gives us a single number list that represents the meaning of the entire sentence.

3. Word2Vec
Word2Vec is a popular method for creating these word embeddings. It’s a machine learning model that "learns" the meanings of words based on their context — the words they usually appear with. Here’s a simplified way to understand it:

Imagine reading lots of books. Over time, you’ll notice patterns like:

“dog” often appears near “bark” or “walk.”
“cat” often appears near “purr” or “nap.”
Word2Vec uses patterns like these to create a numeric representation (embedding) for each word. These embeddings capture relationships between words. So, if we trained Word2Vec on a lot of English text, it would learn that "king" and "queen" are related in the same way as "man" and "woman," for example.

In this project, we’re using Word2Vec to turn each word in our sentences into a list of numbers. Then, by averaging those numbers across all words in a sentence, we create an embedding for the entire sentence.

How It All Works Together in the Code
Here’s the step-by-step flow:

Preprocessing: We start by cleaning up the sentences — removing common words like “the” and “is” and punctuation, so we only focus on the important words.

Training Word2Vec: We feed our cleaned sentences into the Word2Vec model, which learns embeddings (numeric representations) for each word based on the surrounding words.

Sentence Embeddings: For each sentence, we take the embeddings of all the words in the sentence and average them to create a single list of numbers that represents the entire sentence.

Similarity Calculation: We calculate how similar two sentences are by comparing their embeddings. Here, we use cosine similarity, which is a way to measure the angle between two vectors (the lists of numbers representing each sentence). If the vectors are pointing in the same direction, they’re considered similar.

SVR Model: We take the similarity scores and train the SVR model. The SVR model learns to predict how similar two sentences are, based on the cosine similarity of their embeddings.

Evaluation: Finally, we test the model to see how well it can predict similarity scores for new pairs of sentences.

In summary:

Word2Vec: Learns numeric representations for words based on their meanings.
Embedding: Turns words (or sentences) into lists of numbers so a computer can understand them.
SVR: A model that learns from these numbers and can predict similarity between sentences.
By putting these together, we can build a system that automatically understands how close in meaning two short texts are.